{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZO7czJLmMwi++dQ2cgdpv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaddamRafiq/Model-Engineering/blob/main/UseCase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L1AK2B5wqaY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'PSP_Jan_Feb_2019.xlsx'  # Replace with your file path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())\n",
        "\n",
        "# Display data info\n",
        "print(data.info())\n",
        "\n",
        "# Summary statistics\n",
        "print(data.describe())\n",
        "\n",
        "\n",
        "# Plot distribution of transaction amounts\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['amount'], kde=True)\n",
        "plt.title('Distribution of Transaction Amounts')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Plot success rate by PSP\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='PSP', y='success', data=data, estimator=lambda x: len(x) / len(data) * 100)\n",
        "plt.title('Success Rate by PSP')\n",
        "plt.xlabel('Payment Service Provider')\n",
        "plt.ylabel('Success Rate (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Assuming 'data' is a DataFrame containing your dataset\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_data = data.select_dtypes(include=[float, int])\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = numeric_data.corr()\n",
        "\n",
        "# Plot heatmap of correlations\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Drop redundant columns and handle missing values\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "data = data.drop_duplicates()\n",
        "print(f\"Data shape after cleaning: {data.shape}\")\n",
        "\n",
        "# Feature engineering - Extract transaction hour\n",
        "data['transaction_hour'] = pd.to_datetime(data['tmsp']).dt.hour\n",
        "data = data.drop(columns=['tmsp'])  # Drop original timestamp column\n",
        "print(data.head())\n",
        "\n",
        "\n",
        "\n",
        "# Encoding categorical variables\n",
        "data_encoded = pd.get_dummies(data, columns=['country', 'PSP', 'card'], drop_first=True)\n",
        "\n",
        "# Feature scaling for the 'amount' column\n",
        "scaler = StandardScaler()\n",
        "data_encoded['amount'] = scaler.fit_transform(data_encoded[['amount']])\n",
        "\n",
        "# Display the first few rows after transformation\n",
        "print(data_encoded.head())\n",
        "\n",
        "\n",
        "\n",
        "# Define the target variable and features\n",
        "X = data_encoded.drop('success', axis=1)\n",
        "y = data_encoded['success']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Check the shapes of the splits\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "# Train the baseline model\n",
        "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "\n",
        "# Evaluate the baseline model\n",
        "print(\"Baseline Model Performance:\")\n",
        "print(classification_report(y_test, y_pred_baseline))\n",
        "\n",
        "\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Random Forest model\n",
        "print(\"Random Forest Model Performance:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', 'log2']  # Replaced 'auto' with 'sqrt'\n",
        "}\n",
        "\n",
        "# Assuming rf_model is a RandomForestClassifier instance\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='f1')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_pred_tuned_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned Random Forest model\n",
        "print(\"Tuned Random Forest Model Performance:\")\n",
        "print(classification_report(y_test, y_pred_tuned_rf))\n",
        "\n",
        "\n",
        "# Confusion matrix\n",
        "confusion = confusion_matrix(y_test, y_pred_tuned_rf)\n",
        "print(f\"Confusion Matrix:\\n{confusion}\")\n",
        "\n",
        "# Key metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_tuned_rf)\n",
        "precision = precision_score(y_test, y_pred_tuned_rf)\n",
        "recall = recall_score(y_test, y_pred_tuned_rf)\n",
        "f1 = f1_score(y_test, y_pred_tuned_rf)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "importances = best_rf_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importances_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 10 Feature Importances:\")\n",
        "print(feature_importances_df.head(10))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances in Tuned Random Forest')\n",
        "plt.show()"
      ]
    }
  ]
}